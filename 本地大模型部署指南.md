# 本地大模型部署指南 - Ollama

本指南将帮助你在Windows系统上部署本地大模型，并将其集成到缺陷检测系统中。

## 📋 目录

1. [为什么选择本地部署](#为什么选择本地部署)
2. [安装Ollama](#安装ollama)
3. [下载并运行模型](#下载并运行模型)
4. [系统配置说明](#系统配置说明)
5. [测试验证](#测试验证)
6. [常见问题](#常见问题)

---

## 🎯 为什么选择本地部署

✅ **优势：**
- **完全免费**：无需付费API调用
- **数据隐私**：数据不会上传到云端
- **无网络依赖**：离线也能使用
- **响应更快**：本地推理，无网络延迟
- **无限调用**：不受API配额限制

⚠️ **要求：**
- **内存**：建议16GB以上（8GB也可运行小模型）
- **磁盘**：至少10GB空闲空间（存储模型）
- **显卡**：有NVIDIA显卡更好（可选，CPU也能运行）

---

## 📥 安装Ollama

### 第一步：下载Ollama

1. 访问官网：https://ollama.com/download
2. 点击 **Download for Windows** 下载安装包（约750MB）
3. 支持Windows 10/11 64位系统

### 第二步：安装Ollama

1. 双击下载的 `OllamaSetup.exe`
2. 按照向导完成安装（默认安装到C盘）
3. 安装完成后，Ollama会自动在后台运行

### 第三步：验证安装

打开 **PowerShell** 或 **命令提示符**，输入：

```powershell
ollama --version
```

如果显示版本号，说明安装成功！

---

## 🚀 下载并运行模型

### 推荐模型列表

| 模型名称 | 大小 | 内存需求 | 特点 | 推荐度 |
|---------|------|---------|------|--------|
| `qwen2.5:7b` | 4.7GB | 8GB+ | 阿里通义千问2.5，中文优秀 | ⭐⭐⭐⭐⭐ |
| `qwen2.5:14b` | 9GB | 16GB+ | 更强大的千问模型 | ⭐⭐⭐⭐ |
| `llama3.1:8b` | 4.7GB | 8GB+ | Meta最新模型 | ⭐⭐⭐⭐ |
| `qwen2.5:3b` | 2GB | 4GB+ | 轻量级，适合低配置 | ⭐⭐⭐ |

### 下载模型（推荐qwen2.5:7b）

在PowerShell中运行：

```powershell
ollama pull qwen2.5:7b
```

下载过程可能需要10-30分钟（取决于网速）。

### 测试模型

下载完成后，测试运行：

```powershell
ollama run qwen2.5:7b
```

你会进入交互界面，可以输入问题测试：

```
>>> 你好，请介绍一下你自己
>>> /bye  # 退出
```

---

## ⚙️ 系统配置说明

项目已经配置好了，默认使用本地Ollama模型。配置文件位于：

**`defectDetection/src/main/resources/application.yaml`**

```yaml
# AI模型配置
ai:
  # 模式选择: local(本地Ollama) 或 cloud(阿里云)
  mode: local
  
  # 本地Ollama配置
  local:
    url: http://localhost:11434/v1/chat/completions
    model: qwen2.5:7b  # 推荐模型: qwen2.5:7b, qwen2.5:14b, llama3.1:8b
  
  # 阿里云通义千问配置（备用）
  cloud:
    url: https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
    key: sk-your-api-key
    model: qwen-turbo
```

### 切换到不同的本地模型

如果你下载了其他模型，只需修改 `model` 字段：

```yaml
local:
  model: qwen2.5:14b  # 改成你下载的模型
```

### 切换回云端模式

如果需要使用阿里云API，修改mode：

```yaml
ai:
  mode: cloud  # 从 local 改为 cloud
```

---

## ✅ 测试验证

### 1. 确保Ollama运行中

在PowerShell中检查Ollama状态：

```powershell
# 查看已安装的模型
ollama list

# 如果Ollama服务未运行，手动启动
ollama serve
```

### 2. 启动后端服务

在项目根目录运行：

```powershell
cd defectDetection
mvn spring-boot:run
```

查看日志，应该会看到：

```
使用本地Ollama模型: qwen2.5:7b
```

### 3. 测试AI分析功能

1. 打开前端页面
2. 在历史检测记录中，点击 **"AI分析"** 按钮
3. 查看是否能正常返回分析结果

---

## 🔧 常见问题

### Q1: 下载模型太慢怎么办？

**A:** 可以使用国内镜像加速：

```powershell
# 设置环境变量（临时）
$env:OLLAMA_HOST="https://ollama.ai"

# 然后再下载模型
ollama pull qwen2.5:7b
```

### Q2: 提示"connection refused"错误

**A:** Ollama服务未启动，手动启动：

```powershell
ollama serve
```

保持这个窗口打开，或者将Ollama设置为开机自启。

### Q3: 内存不够，模型运行慢

**A:** 使用更小的模型：

```powershell
# 下载轻量级模型（只需2GB内存）
ollama pull qwen2.5:3b
```

然后修改配置文件：
```yaml
local:
  model: qwen2.5:3b
```

### Q4: 如何查看模型存储位置？

**A:** Windows默认存储在：
```
C:\Users\你的用户名\.ollama\models
```

### Q5: 如何删除不用的模型？

**A:** 
```powershell
ollama rm qwen2.5:7b
```

### Q6: 推理速度太慢

**A:** 几种优化方案：
1. **使用GPU加速**（需要NVIDIA显卡）：Ollama会自动检测并使用
2. **关闭其他占用内存的程序**
3. **使用更小的模型**（如3b版本）
4. **增加系统内存**

### Q7: 如何切换回云端API？

**A:** 修改 `application.yaml`：
```yaml
ai:
  mode: cloud  # 改为cloud
```

然后重启后端服务。

---

## 🎓 进阶配置

### 查看Ollama所有可用模型

访问：https://ollama.com/library

或在命令行中：
```powershell
ollama list
```

### 自定义模型参数

创建 `Modelfile`：
```
FROM qwen2.5:7b
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_ctx 4096
```

然后创建自定义模型：
```powershell
ollama create my-qwen -f Modelfile
```

### 设置Ollama开机自启

1. 按 `Win + R` 输入 `shell:startup`
2. 创建快捷方式指向：`C:\Users\你的用户名\AppData\Local\Programs\Ollama\ollama.exe`

---

## 📞 获取帮助

- **Ollama官方文档**: https://github.com/ollama/ollama
- **Ollama中文社区**: https://ollama.ai
- **项目Issues**: 提交到项目的GitHub仓库

---

## 🎉 完成！

现在你已经成功部署了本地大模型！你的缺陷检测系统可以：
- ✅ 完全离线运行AI分析
- ✅ 无限次调用，完全免费
- ✅ 数据完全保密，不上传云端

享受你的本地AI助手吧！🚀
